# -*- coding: utf-8 -*-
"""classification3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UTqCSwNWTel_gM3caA_7xZxuuCSWsU9Z
"""

import pandas as pd

data_info = pd.read_csv('lending_club_info.csv',index_col='LoanStatNew')
data_info

def feat_info(col_name):
    print(data_info.loc[col_name]['Description'])

feat_info('earliest_cr_line')

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('lending_club_loan_two.csv')

df.info()

sns.countplot(x='loan_status',data=df)

plt.figure(figsize=(12,4))
sns.histplot(df['loan_amnt'],bins=40)
plt.xlim(0,45000)

plt.figure(figsize=(12,7))
sns.heatmap(df.corr(),annot=True,cmap='viridis')
plt.ylim(10, 0)

print("Loan Amount: ") 
feat_info('loan_amnt')

print("Installment: ")
feat_info('installment')

plt.figure(figsize=(8,6))
sns.scatterplot(x='installment',y='loan_amnt',data=df,alpha=0.5)

sns.boxplot(x='loan_status',y='loan_amnt',data=df)

df.groupby('loan_status')['loan_amnt'].describe()

df['loan_repaid'] = df['loan_status'].map({'Fully Paid':1,'Charged Off':0})
df[['loan_repaid','loan_status']]

df.corr()['loan_repaid'].sort_values().drop('loan_repaid').plot(kind='bar')

print("mort_acct: ") 
feat_info('mort_acc')
print("\n")
print("annual_inc: ")
feat_info('annual_inc')
print("\n")
print("int_rate: ") 
feat_info('int_rate')
print("\n")
print("revol_util: ") 
feat_info('revol_util')
print("\n")
print("dti: ") 
feat_info('dti')

df.head()

df.isnull().sum()

100* df.isnull().sum()/len(df)

print('emp_title :')
feat_info('emp_title')
print('\n')
print('emp_length :')
feat_info('emp_length')

df['emp_title'].value_counts()

df = df.drop('emp_title',axis=1)

df['emp_length'].dropna().unique()

emp_length_order = [ '< 1 year','1 year','2 years','3 years','4 years','5 years','6 years','7 years','8 years','9 years','10+ years']

plt.figure(figsize=(12,4))
sns.countplot(x='emp_length',data=df,order=emp_length_order)

plt.figure(figsize=(12,4))
sns.countplot(x='emp_length',data=df,order=emp_length_order,hue='loan_status')

emp_co = df[df['loan_status']=="Charged Off"].groupby("emp_length").count()['loan_status']
emp_fp = df[df['loan_status']=="Fully Paid"].groupby("emp_length").count()['loan_status']
emp_len = round(emp_co/emp_fp *100,2)
emp_len

df = df.drop('emp_length',axis=1)

df = df.drop('title',axis=1)

df['mort_acc'].value_counts()

print("Correlation with the mort_acc column")
df.corr()['mort_acc'].sort_values()

print("Mean of mort_acc column per total_acc")
df.groupby('total_acc').mean()['mort_acc']

total_acc_avg = df.groupby('total_acc').mean()['mort_acc']
def fill_mort_acc(total_acc,mort_acc):
    '''
    Accepts the total_acc and mort_acc values for the row.
    Checks if the mort_acc is NaN , if so, it returns the avg mort_acc value
    for the corresponding total_acc value for that row.
    
    total_acc_avg here should be a Series or dictionary containing the mapping of the
    groupby averages of mort_acc per total_acc values.
    '''
    
    
    if np.isnan(mort_acc):
        return total_acc_avg[total_acc]
    else:
        return mort_acc

df['mort_acc'] = df.apply(lambda x: fill_mort_acc(x['total_acc'], x['mort_acc']), axis=1)

df = df.dropna()
df.isnull().sum()

df.select_dtypes(['object']).columns

df['term'].value_counts()

df['term'] = df['term'].apply(lambda term: int(term[:3]))

df = df.drop('grade',axis=1)
dummies = pd.get_dummies(df[['verification_status', 'application_type','initial_list_status','purpose' ,'sub_grade']],drop_first=True)

df = df.drop(['verification_status', 'application_type','initial_list_status','purpose','sub_grade'],axis=1)

df = pd.concat([df,dummies],axis=1)

df

df['home_ownership'].value_counts()

df['home_ownership']=df['home_ownership'].replace(['NONE', 'ANY'], 'OTHER')

dummies = pd.get_dummies(df['home_ownership'],drop_first=True)
df = df.drop('home_ownership',axis=1)
df = pd.concat([df,dummies],axis=1)

df['address'].value_counts()

df['zip_code'] = df['address'].apply(lambda address:address[-5:])
dummies = pd.get_dummies(df['zip_code'],drop_first=True)
df = df.drop(['zip_code','address'],axis=1)
df = pd.concat([df,dummies],axis=1)

df['issue_d']

feat_info('issue_d')

df = df.drop('issue_d',axis=1)

df['earliest_cr_line']

df['earliest_cr_year'] = df['earliest_cr_line'].apply(lambda date:int(date[-4:]))
df = df.drop('earliest_cr_line',axis=1)
df = df.drop('loan_status',axis=1)

df.select_dtypes(['object']).columns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


X = df.drop('loan_repaid',axis=1).values
y = df['loan_repaid'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation,Dropout
from tensorflow.keras.constraints import max_norm

model = Sequential()

model.add(Dense(78,  activation='relu'))
model.add(Dropout(0.33))

model.add(Dense(156, activation='relu'))
model.add(Dropout(0.33))

model.add(Dense(78, activation='relu'))
model.add(Dropout(0.33))

model.add(Dense(39, activation='relu'))
model.add(Dropout(0.33))

model.add(Dense(units=1,activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam',metrics='accuracy')

from tensorflow.keras.callbacks import EarlyStopping
early_stop = EarlyStopping(monitor='val_loss', patience=3)

model.fit(x=X_train, 
          y=y_train, 
          epochs=50,
          batch_size=128,
          validation_data=(X_test, y_test),
          callbacks=[early_stop] 
          )

losses = pd.DataFrame(model.history.history)
losses[['loss','val_loss']].plot()

from sklearn.metrics import classification_report,confusion_matrix,plot_confusion_matrix
predictions = model.predict_classes(X_test)
print(confusion_matrix(y_test,predictions))
print(classification_report(y_test,predictions))

import random
random_ind = random.randint(0,len(df))
customer = df.drop('loan_repaid',axis=1).iloc[random_ind]
customer

model.predict_classes(new_customer.values.reshape(1,78))

df.iloc[random_ind]['loan_repaid']

