# -*- coding: utf-8 -*-
"""Regression_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ebgBJvyzsLf63vZaDdfYyx5c_ZNw6wwu
"""

import numpy as np 
import pandas as pd
from pandas.api.types import is_string_dtype
from pandas.api.types import is_numeric_dtype
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
sns.set_style('whitegrid')

df = pd.read_csv('/content/Ames_Housing_Data.csv')

with open('/content/Ames_Housing_Feature_Description.txt','r') as f: 
  print(f.read())

df.head()

df.info()

df.corr()['SalePrice'].sort_values()[:-1]

plt.figure(figsize=(12,6))
sns.histplot(df['SalePrice'])

sns.scatterplot(x='Overall Qual',y='SalePrice',data=df)

sns.scatterplot(x='Gr Liv Area',y='SalePrice',data=df)

outlier = df[(df['Gr Liv Area']>4000) & (df['SalePrice']<400000)].index
df = df.drop(outlier)

fig, axes = plt.subplots(1, 2,figsize=(12,8))

sns.scatterplot(ax=axes[0],x='Overall Qual',y='SalePrice',data=df)
sns.scatterplot(ax=axes[1],x='Gr Liv Area',y='SalePrice',data=df)
plt.tight_layout()

df = df.drop('PID',axis=1)

miss_value = 100*df.isnull().sum()/len(df)

missing = miss_value[miss_value>0].sort_values(ascending=False)

plt.figure(figsize=(8,10))
sns.barplot(x=missing,y=missing.index)
plt.title('Percentage of Missing Value')
plt.xlabel('%')

df['Lot Frontage'].value_counts()

df['Lot Frontage'] = df['Lot Frontage'].fillna(df['Lot Frontage'].median())

miss_value = 100*df.isnull().sum()/len(df)

missing = miss_value[miss_value>0].sort_values(ascending=False)

plt.figure(figsize=(8,10))
sns.barplot(x=missing,y=missing.index)
plt.title('Percentage of Missing Value')
plt.xlabel('%')

for col in df.columns:
  if is_numeric_dtype(df['{}'.format(col)]):
    df['{}'.format(col)] = df['{}'.format(col)].fillna(0)
  else:
    df['{}'.format(col)] = df['{}'.format(col)].fillna('None')

miss_value = 100*df.isnull().sum()/len(df)
missing = miss_value[miss_value>0].sort_values(ascending=False)
missing

df.head()

df = pd.get_dummies(df,drop_first=True)

df

X = df.drop('SalePrice',axis=1)
y = df['SalePrice']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_X_train = scaler.fit_transform(X_train)
scaled_X_test = scaler.transform(X_test)

from sklearn.linear_model import ElasticNet
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import LinearSVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

def run_model(model,X_train,y_train,X_test,y_test):
  model.fit(X_train,y_train)
  preds = model.predict(X_test)
  rmse = np.sqrt(mean_squared_error(y_test,preds))
  mae = mean_absolute_error(y_test,preds)

  print(f'MAE : {mae}')
  print(f'RMSE : {rmse}')
  print(f'Sale Price Mean : {y.mean()}')

elastic = ElasticNet()
params = {'alpha':[0.1,1,5,10,50,100,150,200,250,300,350],
          'l1_ratio':[.1, .5, .7, .9, .95, .99, 1]}

grid = GridSearchCV(elastic,params,scoring='neg_mean_squared_error',cv=10)

run_model(grid,scaled_X_train,y_train,scaled_X_test,y_test)
print(grid.best_params_)

tree = DecisionTreeRegressor()
params = {'criterion':['mse', 'friedman_mse', 'mae', 'poisson'],
          'splitter':['best','random']}

grid = GridSearchCV(tree,params,scoring='neg_mean_squared_error',cv=10)

run_model(grid,scaled_X_train,y_train,scaled_X_test,y_test)
print(grid.best_params_)

linear_svr = LinearSVR()
params = {'C':[0.001,0.01,0.1,0.5,1,2,5,10],
          'epsilon':[0,0.01,0.1,0.5,1]}

grid = GridSearchCV(linear_svr,params,scoring='neg_mean_squared_error',cv=10)

run_model(grid,scaled_X_train,y_train,scaled_X_test,y_test)
print(grid.best_params_)

random_forest = RandomForestRegressor()
params = {'n_estimators':[10]
          }

grid = GridSearchCV(random_forest,params,scoring='neg_mean_squared_error',cv=10)

run_model(grid,scaled_X_train,y_train,scaled_X_test,y_test)
print(grid.best_params_)

from sklearn.ensemble import GradientBoostingRegressor
model = GradientBoostingRegressor(n_estimators=275)

run_model(model,X_train,y_train,X_test,y_test)

from sklearn.ensemble import AdaBoostRegressor
ada = AdaBoostRegressor(n_estimators=275)

run_model(model,X_train,y_train,X_test,y_test)

